{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With Sparkly\n",
    "\n",
    "This tutorial describes how to perform blocking for entity matching using Sparkly. Specifically, we will see how to build in index and then use the index to perform blocking effeciently. We do this with the following steps,\n",
    "\n",
    "0. Setup\n",
    "1. Reading in Data\n",
    "2. Creating an Index Config\n",
    "3. Building an Index\n",
    "4. Creating a Query Spec\n",
    "5. Performing Blocking\n",
    "6. Save Blocking Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 : Setup\n",
    "\n",
    "Before getting started we first need to install the requirements for Sparkly. \n",
    "\n",
    "## Install PyLucene\n",
    "\n",
    "To install PyLucene see [PyLucene docs](https://lucene.apache.org/pylucene/install.html) \n",
    "\n",
    "## Install this Library\n",
    "\n",
    "To install Sparkly, simply run the following pip command in the root directory of this repo.    \n",
    "    \n",
    "```bash\n",
    "$ python3 -m pip install .\n",
    "``` \n",
    "\n",
    "Now that we have the dependencies installed, we can import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark imports for reading data\n",
    "from pyspark.sql import SparkSession    \n",
    "import pyspark.sql.functions as F\n",
    "# sparkly imports \n",
    "from sparkly.index import LuceneIndex    \n",
    "from sparkly.index_config import IndexConfig  \n",
    "from sparkly.query_generator import QuerySpec    \n",
    "from sparkly.search import Searcher \n",
    "# other python utilities\n",
    "from pathlib import Path  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Reading in Data\n",
    "\n",
    "Now that we have all of the necessary packages imported, we can read in the data using PySpark. Note that for this example, we are running the SparkContext locally and reading the data from local files included in the repo. \n",
    "For production applications spark will likely be run in distributed mode and the data will be read from HDFS or a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the test data    \n",
    "data_path = Path('./data/abt_buy/').absolute()    \n",
    "# table to be indexed    \n",
    "table_a_path = data_path / 'table_a.parquet'    \n",
    "# table for searching    \n",
    "table_b_path = data_path / 'table_b.parquet'    \n",
    "# the ground truth    \n",
    "gold_path = data_path / 'gold.parquet'  \n",
    "\n",
    "# initialize a local spark context    \n",
    "spark = SparkSession.builder\\\n",
    "                    .master('local[*]')\\\n",
    "                    .appName('Sparkly Example')\\\n",
    "                    .getOrCreate()    \n",
    "# read all the data as spark dataframes    \n",
    "table_a = spark.read.parquet(f'file://{str(table_a_path)}')    \n",
    "table_b = spark.read.parquet(f'file://{str(table_b_path)}')    \n",
    "gold = spark.read.parquet(f'file://{str(gold_path)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we have our data stored in parquet files, however Sparkly operates on PySpark and Pandas dataframes, so any file format that be read by PySpark or Pandas can be used. For example, if your data is in csv format it can be read as follows,\n",
    "```python\n",
    "table_a = spark.read.csv('file:///ABSOLUTE/PATH/TO/CSV/FILE.csv')\n",
    "```\n",
    "\n",
    "or by reading it into a Pandas dataframe and then converting to a PySpark dataframe.\n",
    "\n",
    "```python\n",
    "pdf = pd.read_csv('/ABSOLUTE/PATH/TO/CSV/FILE.csv')\n",
    "table_a = spark.createDataFrame(pdf)\n",
    "```\n",
    "\n",
    "Note that PySpark will try to infer the schema of `pdf` when calling `createDataFrame`. In some cases this may fail and require manually providing the schema as an argument, see the [SparkSession.createDataFrame docs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.createDataFrame.html) for more details. See [DataFrameReader docs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.html#pyspark.sql.DataFrameReader) and [DataFrameWriter docs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html) for more information on reading and writing data with PySpark and [Pandas input/output docs](https://pandas.pydata.org/docs/reference/io.html) for reading and writing data with Pandas.\n",
    "\n",
    "In general if your data is stored in files (as opposed to in a database) we strongly recommend that you use parquet as it is significantly faster to read and write, has a more expressive data model, and is strongly typed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : Creating an Index Config\n",
    "\n",
    "Next we need to define how we want to index the data. To do this we need to define sub indexes, which are specified by a field in the indexed dataframe with an analyzer. Here we are going to index two fields and each with two analyzers to create a total of four searchable sub indexes. First we define the index config as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the analyzers used to convert the text into tokens for indexing    \n",
    "# see LuceneIndex.ANALYZERS.keys() for currently implemented analyzers    \n",
    "analyzers = ['3gram', 'standard']\n",
    "# the index config, '_id' column will be used as the unique     \n",
    "# id column in the index. Note id_col must be an integer (32 or 64 bit)    \n",
    "config = IndexConfig(id_col='_id')    \n",
    "# add the 'name' column to be indexed with analyzers above                                                                                                  \n",
    "# note that this will create two sub indexes name.3gram and name.standard                                                                                   \n",
    "# which can be searched independently                                                                                                                       \n",
    "config.add_field('name', analyzers)                                                                                                                         \n",
    "# do the same for the description                                                                                                                           \n",
    "config.add_field('description', analyzers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : Building an Index\n",
    "\n",
    "Now that we have defined the index config, we can build the index by specifying the location (on the local filesystem) where we want to build the index along with how we want to index the data using the config we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new index stored at /tmp/example_index/                                                                                                          \n",
    "index = LuceneIndex('/tmp/example_index/', config)                                                                                                          \n",
    "# index the records from table A according to the config we created above                                                                                   \n",
    "index.upsert_docs(table_a)                                                                                                                                  \n",
    "# this index now has 4 searchable subindexes each named '<FIELD_NAME>.<ANALYZER>', specifically                                                             \n",
    "# 'name.3gram', 'name.standard', 'description.3gram', and 'description.standard'                                                                            \n",
    "                                                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 : Creating a Query Spec\n",
    "\n",
    "Next we need to define how we are going to block using the index that we just built. To do this we create a `QuerySpec`. Notice that we don't need to use all of the subindexes in the index that we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass a mapping of {<SEARCH FIELD> -> {<SUBINDEX NAME>, ...}}                                                                                              \n",
    "# to create a QuerySpec which will specify how queries should be created for documents                                                                      \n",
    "query_spec = QuerySpec({                                                                                                                                    \n",
    "                # use name from table b to search name.3gram and description.standard in the index                                                          \n",
    "                # notice that you can use any field to search any subindex, although                                                                        \n",
    "                # typically you will just want to search the subindexes created by with                                                                     \n",
    "                # the same column                                                                                                                           \n",
    "                'name' : {'name.3gram', 'description.standard'},                                                                                            \n",
    "                # use description from table_b to search description.standard in the index                                                                  \n",
    "                'description' : {'description.standard'}                                                                                                    \n",
    "            })                                                                                                                                              \n",
    "                                                                                                                                                            \n",
    "# kwargs can also be used like a python dict                                                                                                                \n",
    "# this is equivalent to the spec above                                                                                                                      \n",
    "query_spec = QuerySpec(                                                                                                                                     \n",
    "                name = {'name.3gram', 'description.standard'},                                                                                              \n",
    "                description = {'description.standard'}                                                                                                      \n",
    "            )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we simply we want to use all the subindexes we created, we can use the following method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all subindexes\n",
    "query_spec = index.get_full_query_spec()\n",
    "# Equivalent to (for this index)\n",
    "query_spec = QuerySpec({                                                                                                                                                                                                                                                        \n",
    "                'name' : {'name.3gram', 'description.standard'},                                                                                            \n",
    "                'description' : {'description.3gram', 'description.standard'}                                                                                                    \n",
    "            })  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 : Performing Blocking\n",
    "\n",
    "Now that we have read in the data, built an index on the data, and defined how we want to block using the index, we can now perform blocking. We do this by using the `Searcher` class which will handle most of the boilerplate code for doing blocking. The last thing that we need to specify is the id column for the search dataframe and the maximum number of candidates to return per search record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of candidates returned per record    \n",
    "limit = 50    \n",
    "# create a searcher for doing bulk search using our index\n",
    "searcher = Searcher(index)\n",
    "# search the index with table b\n",
    "candidates = searcher.search(table_b, query_spec, id_col='_id', limit=limit).cache()\n",
    "\n",
    "candidates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the recall of the candidate set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output is rolled up as \n",
    "# search record id -> (indexed ids + scores + search time)\n",
    "#\n",
    "# explode the results to compute recall\n",
    "pairs = candidates.select(\n",
    "                    F.explode('ids').alias('a_id'),\n",
    "                    F.col('_id').alias('b_id')\n",
    "                )\n",
    "# number of matches found\n",
    "true_positives = gold.intersect(pairs).count()\n",
    "# precentage of matches found\n",
    "recall = true_positives / gold.count()\n",
    "\n",
    "print(f'true_positives : {true_positives}')\n",
    "print(f'recall : {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 : Saving Blocking Output\n",
    "\n",
    "Finally, we can save the output of blocking to the local filesystem. To do this we simply convert the Spark DataFrame into a Pandas DataFrame and write it using a Pandas DataFrame method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = candidates.toPandas()\n",
    "df.to_parquet('./out.parquet')\n",
    "# remove candidates from Spark cache\n",
    "candidates.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
