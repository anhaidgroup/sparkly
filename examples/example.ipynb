{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With Sparkly\n",
    "\n",
    "This tutorial describes how to perform blocking for entity matching using Sparkly. Specifically, we will see how to build in index and then use the index to perform blocking effeciently. We do this with the following steps,\n",
    "\n",
    "0. Setup\n",
    "1. Reading in Data\n",
    "2. Creating an Index Config\n",
    "3. Building an Index\n",
    "4. Creating a Query Spec\n",
    "5. Performing Blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 : Setup\n",
    "\n",
    "Before getting started we first need to install the requirements for Sparkly. \n",
    "\n",
    "## Install PyLucene\n",
    "\n",
    "To install PyLucene see [PyLucene docs](https://lucene.apache.org/pylucene/install.html) \n",
    "\n",
    "## Install Other Dependencies \n",
    "\n",
    "This repo has a requirements file which will install the python     \n",
    "packages, to install these dependencies simply use pip    \n",
    "    \n",
    "```bash\n",
    "$ python3 -m pip install -r ./requirements.txt\n",
    "``` \n",
    "\n",
    "Now that we have the dependencies installed, we can import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/derek/quals/src/sparkly_repo/examples\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "# add this git repo to the current path\n",
    "sys.path.append('.')\n",
    "# spark imports for reading data\n",
    "from pyspark.sql import SparkSession    \n",
    "import pyspark.sql.functions as F\n",
    "# sparkly imports \n",
    "from sparkly.index import IndexConfig, LuceneIndex    \n",
    "from sparkly.query_generator import QuerySpec    \n",
    "from sparkly.search import Searcher \n",
    "# other python utilities\n",
    "from pathlib import Path  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Reading in Data\n",
    "\n",
    "Now that we have all of the necessary packages imported, we can read in the data using pyspark. Note that for this example, we are running the spark context locally and reading the data from local files included in the repo. \n",
    "For production applications spark will likely be run in distributed mode and the data will be read from HDFS or a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the test data    \n",
    "data_path = Path('./examples/data/abt_buy/').absolute()    \n",
    "# table to be indexed    \n",
    "table_a_path = data_path / 'table_a.parquet'    \n",
    "# table for searching    \n",
    "table_b_path = data_path / 'table_b.parquet'    \n",
    "# the ground truth    \n",
    "gold_path = data_path / 'gold.parquet'  \n",
    "\n",
    "# initialize a local spark context    \n",
    "spark = SparkSession.builder\\    \n",
    "                    .master('local[*]')\\    \n",
    "                    .appName('Sparkly Example')\\    \n",
    "                    .getOrCreate()    \n",
    "# read all the data as spark dataframes    \n",
    "table_a = spark.read.parquet(f'file://{str(table_a_path)}')    \n",
    "table_b = spark.read.parquet(f'file://{str(table_b_path)}')    \n",
    "gold = spark.read.parquet(f'file://{str(gold_path)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : Creating an Index Config\n",
    "\n",
    "Next we need to define how we want to index the data. To do this we need to define sub indexes, which are specified by a field in the indexed dataframe with an analyzer. Here we are going to index two fields and each with two analyzers to create a total of four searchable sub indexes. First we define the index config as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the analyzers used to convert the text into tokens for indexing    \n",
    "# see LuceneIndex.ANALYZERS.keys() for currently implemented analyzers    \n",
    "analyzers = ['3gram', 'standard']\n",
    "# the index config, '_id' column will be used as the unique     \n",
    "# id column in the index. Note id_col must be an integer (32 or 64 bit)    \n",
    "config = IndexConfig(id_col='_id')    \n",
    "# add the 'name' column to be indexed with analyzers above                                                                                                  \n",
    "# note that this will create two sub indexes name.3gram and name.standard                                                                                   \n",
    "# which can be searched independently                                                                                                                       \n",
    "config.add_field('name', analyzers)                                                                                                                         \n",
    "# do the same for the description                                                                                                                           \n",
    "config.add_field('description', analyzers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : Building an Index\n",
    "\n",
    "Now that we have defined the index config, we can build the index by specifying the location (on the local filesystem) where we want to build the index along with how we want to index the data using the config we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new index stored at /tmp/example_index/                                                                                                          \n",
    "index = LuceneIndex('/tmp/example_index/', config)                                                                                                          \n",
    "# index the records from table A according to the config we created above                                                                                   \n",
    "index.upsert_docs(table_a)                                                                                                                                  \n",
    "# this index now has 4 searchable subindexes each named '<FIELD_NAME>.<ANALYZER>', specifically                                                             \n",
    "# 'name.3gram', 'name.standard', 'description.3gram', and 'description.standard'                                                                            \n",
    "                                                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 : Creating a Query Spec\n",
    "\n",
    "Next we need to define how we are going to block using the index that we just built. To do this we create a `QuerySpec`. Notice that we don't need to use all of the subindexes in the index that we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass a mapping of {<SEARCH FIELD> -> {<SUBINDEX NAME>, ...}}                                                                                              \n",
    "# to create a QuerySpec which will specify how queries should be created for documents                                                                      \n",
    "query_spec = QuerySpec({                                                                                                                                    \n",
    "                # use name from table b to search name.3gram and description.standard in the index                                                          \n",
    "                # notice that you can use any field to search any subindex, although                                                                        \n",
    "                # typically you will just want to search the subindexes created by with                                                                     \n",
    "                # the same column                                                                                                                           \n",
    "                'name' : {'name.3gram', 'description.standard'},                                                                                            \n",
    "                # use description from table_b to search description.standard in the index                                                                  \n",
    "                'description' : {'description.standard'}                                                                                                    \n",
    "            })                                                                                                                                              \n",
    "                                                                                                                                                            \n",
    "# kwargs can also be used like a python dict                                                                                                                \n",
    "# this is equivalent to the spec above                                                                                                                      \n",
    "query_spec = QuerySpec(                                                                                                                                     \n",
    "                name = {'name.3gram', 'description.standard'},                                                                                              \n",
    "                description = {'description.standard'}                                                                                                      \n",
    "            )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we simply we want to use all the subindexes we created, we can use the following method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all subindexes\n",
    "query_spec = index.get_full_query_spec()\n",
    "# Equivalent to (for this index)\n",
    "query_spec = QuerySpec({                                                                                                                                                                                                                                                        \n",
    "                'name' : {'name.3gram', 'description.standard'},                                                                                            \n",
    "                'description' : {'description.3gram', 'description.standard'}                                                                                                    \n",
    "            })  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 : Performing Blocking\n",
    "\n",
    "Now that we have read in the data, built an index on the data, and defined how we want to block using the index, we can now perform blocking. We do this by using the `Searcher` class which will handle most of the boilerplate code for doing blocking. The last thing that we need to specify is the id column for the search dataframe and the maximum number of candidates to return per search record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of candidates returned per record    \n",
    "limit = 50    \n",
    "# create a searcher for doing bulk search using our index\n",
    "searcher = Searcher(index)\n",
    "# search the index with table b\n",
    "candidates = searcher.search(table_b, query_spec, id_col='_id', limit=limit).cache()\n",
    "\n",
    "candidates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the recall of the candidate set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output is rolled up as \n",
    "# search record id -> (indexed ids + scores + search time)\n",
    "#\n",
    "# explode the results to compute recall\n",
    "pairs = candidates.select(\n",
    "                    F.explode('ids').alias('a_id'),\n",
    "                    F.col('_id').alias('b_id')\n",
    "                )\n",
    "# number of matches found\n",
    "true_positives = gold.intersect(pairs).count()\n",
    "# precentage of matches found\n",
    "recall = true_positives / gold.count()\n",
    "\n",
    "print(f'true_positives : {true_positives}')\n",
    "print(f'recall : {recall}')\n",
    "\n",
    "candidates.unpersist()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
